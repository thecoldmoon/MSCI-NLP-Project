{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCI 598: Custom Project\n",
    "## Medical Abstract Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE for preprocessing functions: \n",
    "# Abstract Segmentation NLP notebook (https://www.kaggle.com/anshulmehtakaggl/abstract-segmentation-nlp)\n",
    "\n",
    "# Creating a function to read the txt Files\n",
    "# This function returns all the lines in the txt file as a list\n",
    "def get_lines(filename):\n",
    "    with open(filename,\"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "# Creating a function to read the txt Files\n",
    "# This function returns all the lines in the txt file as a list\n",
    "def get_lines(filename):\n",
    "    with open(filename,\"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "#Preprocessing Functions\n",
    "# Returns a list of dictionaries of abstract's lines\n",
    "# Dict Format --> {'TARGET':'Background/Results/Objetive/Concludion','Text':'The actual statement'}\n",
    "def preprocess_data(filename):\n",
    "    input_lines=get_lines(filename)\n",
    "    #This will be used to separte the abstracts from  one another using String mets\n",
    "    abstract_lines=\"\"\n",
    "    # Empty list of abstracts        \n",
    "    abstract_samples=[]\n",
    "    for line in input_lines:\n",
    "        # Check for a new abstract\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_id=line\n",
    "            # And since we are in a new abstract we will Reset the abstract_lines\n",
    "            abstract_lines=\"\"\n",
    "        # Check for a new line \\n escape seq\n",
    "        elif line.isspace():\n",
    "            # Split the Lines of the abstract and will return a list of one abstract\n",
    "            abstract_line_split=abstract_lines.splitlines()\n",
    "            # Now we have to iterate through this singular abstract\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                #  Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object.\n",
    "                # Create a empty Dict per line\n",
    "                line_data={}\n",
    "                # Split on the tab \\t esc seq\n",
    "                target_text_split=abstract_line.split(\"\\t\")\n",
    "                # Get the Label of the sentence as the Label\n",
    "                line_data[\"target\"]=target_text_split[0]\n",
    "                # Get the Text of the Lien as the Text Key\n",
    "                line_data[\"text\"]=target_text_split[1].lower()\n",
    "                # Also adding the Line Nnumber as it will also aid the model\n",
    "                line_data[\"line_number\"]=abstract_line_number\n",
    "                # Number of Lines in that particular abstract\n",
    "                line_data[\"total_lines\"]=len(abstract_line_split)-1\n",
    "                # Now we have to append them to the absract_samples list\n",
    "                abstract_samples.append(line_data)\n",
    "        # So if both the cases are not there then the line is a labelled sentence\n",
    "        else:\n",
    "            abstract_lines+=line\n",
    "    return abstract_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:/med_abstracts/20k_abstracts_numbers_with_@/dev.txt', 'D:/med_abstracts/20k_abstracts_numbers_with_@/test.txt', 'D:/med_abstracts/20k_abstracts_numbers_with_@/train.txt']\n"
     ]
    }
   ],
   "source": [
    "# Reading in and preprocessing data\n",
    "data_dir = \"D:/med_abstracts/20k_abstracts_numbers_with_@/\"\n",
    "filenames=[data_dir + filename for filename in os.listdir(data_dir)]\n",
    "print(filenames)\n",
    "\n",
    "train_samples=preprocess_data(data_dir+\"train.txt\")\n",
    "val_samples=preprocess_data(data_dir+\"dev.txt\")\n",
    "test_samples=preprocess_data(data_dir+\"test.txt\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to df\n",
    "train_df=pd.DataFrame(train_samples)\n",
    "test_df=pd.DataFrame(test_samples)\n",
    "val_df=pd.DataFrame(val_samples)\n",
    "\n",
    "# Data distribution\n",
    "train_df.target.value_counts()\n",
    "\n",
    "# Isolating the sentences\n",
    "train_sentences=train_df[\"text\"].tolist()\n",
    "test_sentences=test_df[\"text\"].tolist()\n",
    "val_sentences=val_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning the target Labels into Numeric Data\n",
    "# We have 5 main labels -> Background, Objective,Methods, Results, Conclusion\n",
    "# We'll encode them both 1HEC and Simple Numerical\n",
    "\n",
    "# Tensorflow is incompatible with sparse matrices\n",
    "one_hot_encoder=OneHotEncoder(sparse=False)\n",
    "# You should reshape your X to be a 2D array not 1D array. Fitting a model requires requires a 2D array. i.e (n_samples, n_features)\n",
    "train_labels_one_hot=one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "val_labels_one_hot=one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "test_labels_one_hot=one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "train_labels_one_hot,val_labels_one_hot,test_labels_one_hot\n",
    "\n",
    "le=LabelEncoder()\n",
    "train_labels_encoded=le.fit_transform(train_df[\"target\"])\n",
    "test_labels_encoded=le.fit_transform(test_df[\"target\"])\n",
    "val_labels_encoded=le.fit_transform(val_df[\"target\"])\n",
    "\n",
    "# Retieving classes \n",
    "num_classes=len(le.classes_)\n",
    "class_names=le.classes_\n",
    "num_classes,class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text and then create Embeddings\n",
    "\n",
    "# How long is each sentence on average\n",
    "sent_lens=[len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_lens=np.mean(sent_lens)\n",
    "avg_sent_lens\n",
    "# sent_lens\n",
    "#  So we will need Padding and Truncating as the input shapes must be maintained\n",
    "\n",
    "# Calculate the percentile of length of sentences\n",
    "output_seq_len=int(np.percentile(sent_lens,95))\n",
    "output_seq_len\n",
    "# So 95% sentences are in length of 55\n",
    "\n",
    "# Creating a text Vectorization Layer\n",
    "# Mapping our text from words to Numbers\n",
    "# An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. \n",
    "# Vocabulary size in the Research Paper is 68000\n",
    "max_tokens=68000\n",
    "\n",
    "text_vectorizer=TextVectorization(max_tokens=max_tokens,output_sequence_length=output_seq_len)\n",
    "\n",
    "# Adapt the Text Vectorizer to the Training Data\n",
    "# We have to adapt it to only the training data so that val and test data are not seen\n",
    "# Later it can be fitted to the two latter\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64841\n",
      "['', '[UNK]', 'the', 'and', 'of']\n",
      "['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding out how many words are there  in the training vocabulary and which are  most common\n",
    "# Also text vectorizer works pretty straightforwardly, 1 to most common word, 2 to 2nd most common word and so on\n",
    "train_vocab=text_vectorizer.get_vocabulary()\n",
    "# Size of Vocab\n",
    "print(len(train_vocab))\n",
    "# 5 Most Common Words in the Vocab\n",
    "print(train_vocab[:5])\n",
    "# Least common 5 words in the vocab\n",
    "print(train_vocab[-5:])\n",
    "\n",
    "# Get the config of our Text Vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Embedding Layer\n",
    "# More output dims , more emmbedding, more parameters to train\n",
    "# Masking the 0 considering them as padding\n",
    "token_embed=layers.Embedding(input_dim=len(train_vocab),output_dim=128,mask_zero=True,name=\"token_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Fast Loadinng Dataset with tf data API\n",
    "# https://www.tensorflow.org/guide/data_performance\n",
    "# https://www.tensorflow.org/guide/data\n",
    "# Turn our data into Tensorflow datasets\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\n",
    "val_dataset=tf.data.Dataset.from_tensor_slices((val_sentences,val_labels_one_hot))\n",
    "test_dataset=tf.data.Dataset.from_tensor_slices((test_sentences,test_labels_one_hot))\n",
    "train_dataset\n",
    "# <TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)>\n",
    "# Which indicates one Text Sample in first tuple, next tuple is (0,0,0,0,1) -> 1hc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre fetching the data and making them into batches\n",
    "# Pre fetching reduces the Preparation time of Data taken by CPU\n",
    "# Pref-fetching in a Multi-threaded way Reduces time and Increases the amount of data as all cores can be utilized to Prepare the Data\n",
    "# The GPU will do the Computation\n",
    "train_dataset=train_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset=val_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset=test_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train_dataset\n",
    "# Run the Previous steps as well this otherwie the Shapes will not be fixed\n",
    "# PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Model\n",
    "inputs=layers.Input(shape=(1,),dtype=tf.string)\n",
    "text_vectors=text_vectorizer(inputs)\n",
    "token_embedding=token_embed(text_vectors)\n",
    "x=layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(token_embedding)\n",
    "x=layers.GlobalAveragePooling1D()(x)\n",
    "outputs=layers.Dense(num_classes,activation=\"softmax\")(x)\n",
    "# Indirect way of creating the Modelling the op ip\n",
    "model_1=tf.keras.Model(inputs,outputs)\n",
    "# Compiling the Model\n",
    "model_1.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_4 (TextVe (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "token_embedding (Embedding)  (None, 55, 128)           8299648   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 55, 64)            41024     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5627/5627 [==============================] - 610s 108ms/step - loss: 0.6151 - accuracy: 0.7762 - val_loss: 0.5372 - val_accuracy: 0.8090\n",
      "Epoch 2/5\n",
      "5627/5627 [==============================] - 582s 103ms/step - loss: 0.4527 - accuracy: 0.8408 - val_loss: 0.5357 - val_accuracy: 0.8096\n",
      "Epoch 3/5\n",
      "5627/5627 [==============================] - 584s 104ms/step - loss: 0.3725 - accuracy: 0.8724 - val_loss: 0.5692 - val_accuracy: 0.8058\n",
      "Epoch 4/5\n",
      "5627/5627 [==============================] - 640s 114ms/step - loss: 0.3082 - accuracy: 0.8973 - val_loss: 0.6301 - val_accuracy: 0.8001\n",
      "Epoch 5/5\n",
      "5627/5627 [==============================] - 647s 115ms/step - loss: 0.2545 - accuracy: 0.9171 - val_loss: 0.7110 - val_accuracy: 0.7920\n"
     ]
    }
   ],
   "source": [
    "history_model_1=model_1.fit(train_dataset,epochs=5,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 3s 3ms/step - loss: 0.7110 - accuracy: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7110169529914856, 0.7919700741767883]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0300645e-01, 1.2680595e-03, 2.2879089e-01, 5.0042268e-02,\n",
       "        1.6892424e-02],\n",
       "       [5.5768144e-01, 3.1261150e-02, 6.2722201e-03, 3.9774191e-01,\n",
       "        7.0433393e-03],\n",
       "       [3.3693090e-02, 1.3626387e-03, 1.5214458e-03, 9.6337378e-01,\n",
       "        4.9019684e-05],\n",
       "       ...,\n",
       "       [5.3408460e-09, 1.6305681e-07, 2.4358046e-04, 9.0132506e-09,\n",
       "        9.9975628e-01],\n",
       "       [9.9451326e-02, 3.6207575e-01, 2.5237784e-01, 1.8783767e-02,\n",
       "        2.6731133e-01],\n",
       "       [1.7279190e-04, 9.9964249e-01, 1.6835335e-04, 5.8312901e-07,\n",
       "        1.5736716e-05]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making Predictions In terms of Probabilities\n",
    "model_1_prediction_probability=model_1.predict(val_dataset)\n",
    "model_1_prediction_probability\n",
    "# For all 30k statements our Model will output a 5 len list of Prediction Probability\n",
    "# And out of the 5 the index that is higher is the one in which our class thinks the \n",
    "# Sentence belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now converting the Probabilities to classes\n",
    "model_1_prediction=tf.argmax(model_1_prediction_probability,axis=1)\n",
    "model_1_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.19700781146565,\n",
       " 'precision': 0.7896447524512055,\n",
       " 'recall': 0.7919700781146565,\n",
       " 'f1': 0.789347087728524}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# Models for Calculating different evaluation metrics\n",
    "# Returns a dict of different metrics\n",
    "def calculate_results(y_true, y_pred):\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results\n",
    "\n",
    "model_1_results=calculate_results(y_true=val_labels_encoded,y_pred=model_1_prediction)\n",
    "model_1_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfhub_handle_encoder = 'https://tfhub.dev/google/experts/bert/pubmed/2'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length from data notebook\n",
    "SEQ_LENGTH = 128\n",
    "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
    "    \"\"\"Returns Model mapping string features to BERT inputs.\n",
    "\n",
    "    Args:\n",
    "    sentence_features: a list with the names of string-valued features.\n",
    "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
    "\n",
    "    Returns:\n",
    "    A Keras Model that can be called on a list or dict of string Tensors\n",
    "    (with the order or names, resp., given by sentence_features) and\n",
    "    returns a dict of tensors for input to BERT.\n",
    "    \"\"\"\n",
    "\n",
    "    input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "    # Tokenize the text to word pieces.\n",
    "    bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "    tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "    segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "    # Optional: Trim segments in a smart way to fit seq_length.\n",
    "    # Simple cases (like this example) can skip this step and let\n",
    "    # the next step apply a default truncation to approximately equal lengths.\n",
    "    truncated_segments = segments\n",
    "\n",
    "    # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "    # are model-dependent, so this gets loaded from the SavedModel.\n",
    "    packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "    model_inputs = packer(truncated_segments)\n",
    "    return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(train_sentences)\n",
    "test_text = [np.array(['some random test sentence']),\n",
    "             np.array(['another sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/google/experts/bert/pubmed/2\n",
      "Pooled Outputs Shape:(1, 768)\n",
      "Pooled Outputs Values:[ 0.42222086 -0.48337984  0.36544615 -0.8634969  -0.14478478 -0.63755333\n",
      " -0.35506263  0.86173105  0.8384985  -0.6099298   0.70421    -0.10690586]\n",
      "Sequence Outputs Shape:(1, 128, 768)\n",
      "Sequence Outputs Values:[[ 0.45039162 -0.52738523  0.3831572  ... -0.599706   -0.443533\n",
      "  -0.06585392]\n",
      " [-0.6186835  -1.8081663   0.44193897 ... -0.3325248  -0.9120241\n",
      "  -3.5160751 ]\n",
      " [ 0.03471279 -1.3693182   0.9009443  ... -0.23209126 -0.01226728\n",
      "  -2.5252082 ]\n",
      " ...\n",
      " [ 1.1027188  -1.5887632   0.37287942 ... -0.37087232  0.18958531\n",
      "   0.8353863 ]\n",
      " [ 0.9942305  -1.274762    0.27017665 ... -0.462246    0.02420896\n",
      "   0.9638618 ]\n",
      " [ 1.077033   -1.3870149   0.28120837 ... -0.6093174  -0.06627202\n",
      "   0.5650061 ]]\n"
     ]
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
