{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3aa36d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112f8f8b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29159be",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE=68000\n",
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5f52cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(torch.cuda.is_available() )\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bb8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(filename):\n",
    "    with open(filename,\"r\") as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05901de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
    "# LABEL = data.LabelField()\n",
    "\n",
    "# train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "# TEXT.build_vocab(train_data, vectors='glove.6B.100d')\n",
    "\n",
    "# TEXT.vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b630d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Lines in the Training Set\n",
    "pos_data_lines=get_lines(data_dir+\"pos.txt\")\n",
    "neg_data_lines=get_lines(data_dir+\"neg.txt\")\n",
    "sentences = []\n",
    "for txt in pos_data_lines:\n",
    "    line_data = {}\n",
    "    line_data[\"target\"]=\"POSITIVE\"\n",
    "    line_data[\"text\"] = txt\n",
    "    sentences.append(line_data)\n",
    "for txt in neg_data_lines:\n",
    "    line_data = {}\n",
    "    line_data[\"target\"]=\"NEGATIVE\"\n",
    "    line_data[\"text\"] = txt\n",
    "    sentences.append(line_data)\n",
    "random.shuffle(sentences)\n",
    "train_samples = sentences[:round(len(sentences)*0.8)]\n",
    "test_samples = sentences[:round(len(sentences)*0.2)]\n",
    "random.shuffle(sentences)\n",
    "val_samples = sentences[:round(len(sentences)*0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8c7744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentences)\n",
    "train_samples = sentences[:round(len(sentences)*0.8)]\n",
    "test_samples = sentences[:round(len(sentences)*0.2)]\n",
    "random.shuffle(sentences)\n",
    "val_samples = sentences[:round(len(sentences)*0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15e65251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_samples[0][\"target\"]) , type(train_samples[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c9fc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to df\n",
    "train_df=pd.DataFrame(train_samples)\n",
    "test_df=pd.DataFrame(test_samples)\n",
    "val_df=pd.DataFrame(val_samples)\n",
    "\n",
    "# Data distribution\n",
    "train_df.target.value_counts()\n",
    "\n",
    "# Isolating the sentences\n",
    "train_sentences=train_df[\"text\"].tolist()\n",
    "test_sentences=test_df[\"text\"].tolist()\n",
    "val_sentences=val_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13674ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, array(['NEGATIVE', 'POSITIVE'], dtype=object))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning the target Labels into Numeric Data\n",
    "# We have 5 main labels -> Background, Objective,Methods, Results, Conclusion\n",
    "# We'll encode them both 1HEC and Simple Numerical\n",
    "\n",
    "# Tensorflow is incompatible with sparse matrices\n",
    "one_hot_encoder=OneHotEncoder(sparse=False)\n",
    "# You should reshape your X to be a 2D array not 1D array. Fitting a model requires requires a 2D array. i.e (n_samples, n_features)\n",
    "train_labels_one_hot=one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "val_labels_one_hot=one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "test_labels_one_hot=one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n",
    "train_labels_one_hot,val_labels_one_hot,test_labels_one_hot\n",
    "\n",
    "le=LabelEncoder()\n",
    "train_labels_encoded=le.fit_transform(train_df[\"target\"])\n",
    "test_labels_encoded=le.fit_transform(test_df[\"target\"])\n",
    "val_labels_encoded=le.fit_transform(val_df[\"target\"])\n",
    "\n",
    "train_labels_onehot = nn.functional.one_hot(torch.from_numpy(train_labels_encoded).to(torch.int64), num_classes=- 1)\n",
    "val_labels_onehot = nn.functional.one_hot(torch.from_numpy(val_labels_encoded).to(torch.int64), num_classes=- 1)\n",
    "test_labels_onehot = nn.functional.one_hot(torch.from_numpy(test_labels_encoded).to(torch.int64), num_classes=- 1)\n",
    "\n",
    "\n",
    "# Retieving classes \n",
    "num_classes=len(le.classes_)\n",
    "class_names=le.classes_\n",
    "num_classes,class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74adeedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text and then create Embeddings\n",
    "\n",
    "# How long is each sentence on average\n",
    "sent_lens=[len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_lens=np.mean(sent_lens)\n",
    "avg_sent_lens\n",
    "# sent_lens\n",
    "#  So we will need Padding and Truncating as the input shapes must be maintained\n",
    "\n",
    "# Calculate the percentile of length of sentences\n",
    "output_seq_len=int(np.percentile(sent_lens,95))\n",
    "output_seq_len\n",
    "# So 95% sentences are in length of 24\n",
    "\n",
    "# Creating a text Vectorization Layer\n",
    "# Mapping our text from words to Numbers\n",
    "# An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. \n",
    "# Vocabulary size in the Research Paper is 68000\n",
    "max_tokens=68000\n",
    "\n",
    "text_vectorizer=TextVectorization(max_tokens=max_tokens,output_sequence_length=output_seq_len)\n",
    "\n",
    "# Adapt the Text Vectorizer to the Training Data\n",
    "# We have to adapt it to only the training data so that val and test data are not seen\n",
    "# Later it can be fitted to the two latter\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1a1c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68000\n",
      "['', '[UNK]', 'the', 'i', 'and']\n",
      "['somersaut', 'someresearch', 'someplacethe', 'someplacemy', 'someothers']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization_2',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 24,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding out how many words are there  in the training vocabulary and which are  most common\n",
    "# Also text vectorizer works pretty straightforwardly, 1 to most common word, 2 to 2nd most common word and so on\n",
    "train_vocab=text_vectorizer.get_vocabulary()\n",
    "# Size of Vocab\n",
    "print(len(train_vocab))\n",
    "# 5 Most Common Words in the Vocab\n",
    "print(train_vocab[:5])\n",
    "# Least common 5 words in the vocab\n",
    "print(train_vocab[-5:])\n",
    "\n",
    "# Get the config of our Text Vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "312de974",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embed_py = torch.nn.Embedding(num_embeddings=len(train_vocab), embedding_dim=128, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7e52cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for vectorized sentences\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc5ba741",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTextDataset(train_sentences, train_labels_one_hot)\n",
    "val_dataset = CustomTextDataset(val_sentences, val_labels_one_hot)\n",
    "test_dataset = CustomTextDataset(test_sentences, test_labels_one_hot)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=28, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=28, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=28, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d3cd3",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fe2235eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension=128):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(len(train_vocab), 300, padding_idx=0)\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=300,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 2)\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        #packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_input = text_emb\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        #output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = packed_output\n",
    "        \n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out\n",
    "    \n",
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182a8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4d/jvnc9_4j09d7jr9pbgj7t8080000gn/T/ipykernel_2668/2274186237.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data, labels = torch.tensor(data).to(device), torch.tensor(labels).to(device)\n"
     ]
    }
   ],
   "source": [
    "destination_folder = \"./models\"\n",
    "\n",
    "# Training Function\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_loader,\n",
    "          valid_loader = val_loader,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_loader) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, labels in train_loader:   \n",
    "            data = text_vectorizer(data).numpy()\n",
    "            data, labels = torch.tensor(data).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "            output = model(data, output_seq_len)\n",
    "\n",
    "            loss = criterion(output, labels.to(torch.float32))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                  # validation loop\n",
    "                  for data, labels in valid_loader:\n",
    "                        data = text_vectorizer(data).numpy()\n",
    "                        data, labels = torch.tensor(data).to(device), torch.tensor(labels).to(device)\n",
    "                        output = model(data, output_seq_len)\n",
    "\n",
    "                        loss = criterion(output, labels.to(torch.float32))\n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "                    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')\n",
    "    \n",
    "\n",
    "num_epochs = 10\n",
    "model = LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "\n",
    "train(model=model, optimizer=optimizer, scheduler=scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "word_labels = np.unique(train_df[\"target\"])\n",
    "word_labels_short = [word_labels[i][:4] for i in np.arange(len(word_labels))]\n",
    "\n",
    "def evaluate(model, test_loader, version='title', threshold=0.5):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = text_vectorizer(data).numpy()\n",
    "            data, labels = torch.tensor(data).to(device), torch.tensor(labels).to(device)\n",
    "            output = model(data, output_seq_len)\n",
    "\n",
    "            output = (output > threshold).int()\n",
    "            y_pred.extend(output.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[0,1,2,3,4], target_names=word_labels, digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(np.array(y_true).argmax(axis=1), np.array(y_pred).argmax(axis=1), labels=[0,1,2,3,4], normalize='true')\n",
    "    print(cm)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues')\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(word_labels_short)\n",
    "    ax.yaxis.set_ticklabels(word_labels_short)\n",
    "    \n",
    "    \n",
    "best_model = LSTM().to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model, optimizer)\n",
    "evaluate(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a001b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [module for module in model.modules() if not isinstance(module, nn.Sequential)]\n",
    "print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
